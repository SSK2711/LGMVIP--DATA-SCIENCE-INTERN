{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "30bbd51c",
      "metadata": {
        "id": "30bbd51c"
      },
      "source": [
        "LGMVIP - Data Science ( April 2022 )\n",
        "\n",
        "NAME : SHREYAS KSHIRSAGAR\n",
        "\n",
        "Task 8) Next Word Prediction\n",
        "\n",
        "Dataset Link : https://www.canva.com/link?target=https%3A%2F%2Fthecleverprogrammer.com%2F2020%2F07%2F20%2Fnext-word-prediction-model%2F&design=DAEjrwWV35w"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3d18d82",
      "metadata": {
        "id": "d3d18d82"
      },
      "source": [
        "Import these libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7addff04",
      "metadata": {
        "id": "7addff04"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import heapq\n",
        "import warnings as wg\n",
        "wg.filterwarnings(\"ignore\")\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.core import Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f631ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "14f631ac",
        "outputId": "9675da25-e6f3-4e33-dd1f-2602958cbf01"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2af66793e87a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# source text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1661-0.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1661-0.txt'"
          ]
        }
      ],
      "source": [
        "# source text\n",
        "file = open('1661-0.txt', encoding=\"utf8\").read().lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a546709",
      "metadata": {
        "id": "2a546709"
      },
      "outputs": [],
      "source": [
        "print('corpus length:', len(file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14faf902",
      "metadata": {
        "id": "14faf902"
      },
      "outputs": [],
      "source": [
        "file = open(\"1661-0.txt\", \"r\", encoding = \"utf8\")\n",
        "lines = []\n",
        "\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "    \n",
        "#print(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f377e011",
      "metadata": {
        "id": "f377e011"
      },
      "outputs": [],
      "source": [
        "data = \"\"\n",
        "\n",
        "for i in lines:\n",
        "    data = ' '. join(lines)\n",
        "    \n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "data[:360]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "216c4d09",
      "metadata": {
        "id": "216c4d09"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
        "new_data = data.translate(translator)\n",
        "\n",
        "new_data[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ccca9d3",
      "metadata": {
        "id": "6ccca9d3"
      },
      "outputs": [],
      "source": [
        "z = []\n",
        "\n",
        "for i in data.split():\n",
        "    if i not in z:\n",
        "        z.append(i)\n",
        "        \n",
        "data = ' '.join(z)\n",
        "data[:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca97c1a7",
      "metadata": {
        "id": "ca97c1a7"
      },
      "source": [
        "# Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8017ea4",
      "metadata": {
        "id": "a8017ea4"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function.\n",
        "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81ebf6a2",
      "metadata": {
        "id": "81ebf6a2"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d58bbe9",
      "metadata": {
        "id": "2d58bbe9"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(1, len(sequence_data)):\n",
        "    words = sequence_data[i-1:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197fea72",
      "metadata": {
        "id": "197fea72"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0])\n",
        "    y.append(i[1])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"The Data is: \", X[:5])\n",
        "print(\"The responses are: \", y[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6f230b7",
      "metadata": {
        "id": "e6f230b7"
      },
      "outputs": [],
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf0e62cb",
      "metadata": {
        "id": "cf0e62cb"
      },
      "source": [
        "## RNN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be39a12b",
      "metadata": {
        "id": "be39a12b"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80ab0182",
      "metadata": {
        "id": "80ab0182"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40f83f8a",
      "metadata": {
        "id": "40f83f8a"
      },
      "outputs": [],
      "source": [
        "## call back\n",
        "\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto')\n",
        "\n",
        "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
        "\n",
        "logdir='logsnextword1'\n",
        "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97eda942",
      "metadata": {
        "id": "97eda942"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d4f9fc6",
      "metadata": {
        "scrolled": false,
        "id": "3d4f9fc6"
      },
      "outputs": [],
      "source": [
        "model.fit(X, y, epochs=40, batch_size=150, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c743bb8",
      "metadata": {
        "id": "8c743bb8"
      },
      "outputs": [],
      "source": [
        "model = load_model('nextword1.h5')\n",
        "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "\n",
        "  sequence = tokenizer.texts_to_sequences([text])\n",
        "  sequence = np.array(sequence)\n",
        "  preds = np.argmax(model.predict(sequence))\n",
        "  predicted_word = \"\"\n",
        "  \n",
        "  for key, value in tokenizer.word_index.items():\n",
        "      if value == preds:\n",
        "          predicted_word = key\n",
        "          break\n",
        "  \n",
        "  print(predicted_word)\n",
        "  return predicted_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74bf52e0",
      "metadata": {
        "id": "74bf52e0"
      },
      "outputs": [],
      "source": [
        "while(True):\n",
        "  text = input(\"Enter your line: \")\n",
        "  \n",
        "  if text == \"0\":\n",
        "      print(\"Execution completed.....\")\n",
        "      break\n",
        "  \n",
        "  else:\n",
        "      try:\n",
        "          text = text.split(\" \")\n",
        "          text = text[-3:]\n",
        "          print(text)\n",
        "        \n",
        "          Predict_Next_Words(model, tokenizer, text)\n",
        "          \n",
        "      except Exception as e:\n",
        "        print(\"Error occurred: \",e)\n",
        "        continue\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "_LGM_Task8_NextWordPrediction.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}